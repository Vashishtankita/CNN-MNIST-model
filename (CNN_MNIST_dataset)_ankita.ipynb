{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3W5N5c28fRBq",
    "outputId": "8ef1803a-bb5b-4756-e930-bcf531fe219a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
      "Collecting mnist\n",
      "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Installing collected packages: mnist\n",
      "Successfully installed mnist-0.2.2\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow numpy mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDq9bGYHgnoo"
   },
   "source": [
    "Note: We don’t need to install the keras package because it now comes bundled with TensorFlow as its official high-level API! Using TensorFlow’s Keras is now recommended over the standalone keras package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1WJjrrxgnY2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uMxTy_7VfhOl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OqMP5oVjSGP",
    "outputId": "1e9f401b-7439-4b45-ba20-c5d9d44e6a02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the MNIST dataset using keras.datasets\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jHmq1sBj5l-",
    "outputId": "91f53b03-2319-4ec3-b5da-3d705508e315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape) # (60000, 28, 28)\n",
    "print(train_labels.shape) # (60000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV5p9TPigvRg"
   },
   "source": [
    "2. Preparing the Data\n",
    "Before we begin, we’ll normalize the image pixel values from [0, 255] to [-0.5, 0.5] to make our network easier to train (using smaller, centered values usually leads to better results). We’ll also reshape each image from (28, 28) to (28, 28, 1) because Keras requires the third dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YZ9IfliCfhRD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "D0RL4nOnfhTt"
   },
   "outputs": [],
   "source": [
    "# Normalize the images to be in the range [-0.5, 0.5]\n",
    "train_images = (train_images / 255.0) - 0.5\n",
    "test_images = (test_images / 255.0) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EtPeH7ozfhWO"
   },
   "outputs": [],
   "source": [
    "# Reshape the images to include the channel dimension (28, 28, 1)\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qbGNt9NfhY5",
    "outputId": "02597ad5-d033-44bb-a33c-fa446ad19d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape) # (60000, 28, 28, 1)\n",
    "print(test_images.shape)  # (10000, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXZ3zzY1lP8T"
   },
   "source": [
    "We’re ready to start building our CNN!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdaDCshHlQyC"
   },
   "source": [
    "3. Building the Model\n",
    "Every Keras model is either built using the Sequential class, which represents a linear stack of layers, or the functional Model class, which is more customizeable. We’ll be using the simpler Sequential model, since our CNN will be a linear stack of layers.\n",
    "\n",
    "We start by instantiating a Sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BLNUTq2ofhbR"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# WIP\n",
    "model = Sequential([\n",
    "  # layers...\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RZi11WllX6Y"
   },
   "source": [
    "The Sequential constructor takes an array of Keras Layers. We’ll use 3 types of layers for our CNN: Convolutional, Max Pooling, and Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9DMWmt4ifheG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "import mnist\n",
    "# Print the first label\n",
    "print(train_labels[0])\n",
    "# This should print the label of the first training image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize and preprocess the images\n",
    "train_images = (train_images / 255.0) - 0.5\n",
    "test_images = (test_images / 255.0) - 0.5\n",
    "\n",
    "# Reshape the images to include a single channel (for grayscale)\n",
    "train_images = np.expand_dims(train_images, axis=-1)\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=3, validation_data=(test_images, test_labels))\n",
    "model.save_weights('cnn.h5')\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "# Define your model architecture for loading weights\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Load weights into the model\n",
    "model.load_weights('cnn.h5')\n",
    "\n",
    "# Now you can use the model for predictions or further training\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  Conv2D(num_filters, filter_size),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "# These can be changed, too!\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "\n",
    "model = Sequential([\n",
    "  # See https://keras.io/layers/convolutional/#conv2d for more info.\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "  ),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GetJJVWulhA8"
   },
   "source": [
    "num_filters, filter_size, and pool_size are self-explanatory variables that set the hyperparameters for our CNN.\n",
    "The first layer in any Sequential model must specify the input_shape, so we do so on Conv2D. Once this input shape is specified, Keras will automatically infer the shapes of inputs for later layers.\n",
    "The output Softmax layer has 10 nodes, one for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdB0e_jNljcY"
   },
   "source": [
    "4. Compiling the Model\n",
    "Before we can begin training, we need to configure the training process. We decide 3 key factors during the compilation step:\n",
    "\n",
    "The optimizer. We’ll stick with a pretty good default: the Adam gradient-based optimizer. Keras has many other optimizers you can look into as well.\n",
    "The loss function. Since we’re using a Softmax output layer, we’ll use the Cross-Entropy loss. Keras distinguishes between binary_crossentropy (2 classes) and categorical_crossentropy (>2 classes), so we’ll use the latter. See all Keras losses.\n",
    "A list of metrics. Since this is a classification problem, we’ll just have Keras report on the accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bRwjRFUEfhhg"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdQq6n1ilpg_"
   },
   "source": [
    "5. Training the Model\n",
    "Training a model in Keras literally consists only of calling fit() and specifying some parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxkmEpXzluna"
   },
   "source": [
    "There’s one thing we have to be careful about: Keras expects the training targets to be 10-dimensional vectors, since there are 10 nodes in our Softmax output layer. Right now, our train_labels and test_labels arrays contain single integers representing the class for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IEvyL04Mlql0"
   },
   "outputs": [],
   "source": [
    "import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pd1LMfXYmEtc",
    "outputId": "d4e1b5aa-2858-49a9-99ec-a674db705e34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Print the first label\n",
    "print(train_labels[0])\n",
    "# This should print the label of the first training image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhcn1bHWmdsP"
   },
   "source": [
    "Conveniently, Keras has a utility method that fixes this exact issue: to_categorical. It turns our array of class integers into an array of one-hot vectors instead.\n",
    "For example, 2 would become [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] (it’s zero-indexed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oizCL8kCmYml"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzLtRQrFmgyG",
    "outputId": "2620d370-b80c-4737-acff-4a26b2ac2ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.3346 - accuracy: 0.9035 - val_loss: 0.1940 - val_accuracy: 0.9438\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1609 - accuracy: 0.9542 - val_loss: 0.1319 - val_accuracy: 0.9603\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1184 - accuracy: 0.9656 - val_loss: 0.1150 - val_accuracy: 0.9648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x78a7fbb4d4e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "_P-tYdWkmoEb"
   },
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tw6x3UkCp4no",
    "outputId": "b6888a5d-6b7d-4cf9-ad1c-cb8fddc6da7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.2102 - accuracy: 0.9405 - val_loss: 0.0822 - val_accuracy: 0.9734\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0772 - accuracy: 0.9775 - val_loss: 0.0701 - val_accuracy: 0.9776\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0602 - accuracy: 0.9821 - val_loss: 0.0610 - val_accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x78a7d1ee39a0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize and preprocess the images\n",
    "train_images = (train_images / 255.0) - 0.5\n",
    "test_images = (test_images / 255.0) - 0.5\n",
    "\n",
    "# Reshape the images to include a single channel (for grayscale)\n",
    "train_images = np.expand_dims(train_images, axis=-1)\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=3, validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptmwlQisqilA"
   },
   "source": [
    "6. Using the Model\n",
    "Now that we have a working, trained model, let’s put it to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "NsIPQS5fqgDY"
   },
   "outputs": [],
   "source": [
    "model.save_weights('cnn.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "AliGzZkwqf3o"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "# Define your model architecture for loading weights\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Load weights into the model\n",
    "model.load_weights('cnn.h5')\n",
    "\n",
    "# Now you can use the model for predictions or further training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "An8Wfjsaqfra",
    "outputId": "5e624d06-c94c-4949-8fdd-03088539884b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step\n",
      "[8 8 8 8 8]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVs3F34LrzT_"
   },
   "source": [
    "8. Extensions\n",
    "There’s much more we can do to experiment with and improve our network - in this official Keras MNIST CNN example, they achieve 99 test accuracy after 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "a7LAJAcsr0US"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  Conv2D(num_filters, filter_size),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "sO1H2d2Yr09U"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "2es2Tclcr05w"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "wzKgtHu1r9Ww"
   },
   "outputs": [],
   "source": [
    "# These can be changed, too!\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "\n",
    "model = Sequential([\n",
    "  # See https://keras.io/layers/convolutional/#conv2d for more info.\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "  ),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpqGGBq7r9TM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbkPhbhwr9QO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4yrE9H4r9L0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
